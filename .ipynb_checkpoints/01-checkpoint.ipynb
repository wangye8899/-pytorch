{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Pytorch的基本使用\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 1. autograd例子\n",
    "# create tensors 创建张量\n",
    "\n",
    "# requires_grad:设置为True是允许精细的排除子图 提高计算效率\n",
    "x = torch.tensor(1.,requires_grad=True)\n",
    "w = torch.tensor(2.,requires_grad=True)\n",
    "b = torch.tensor(3.,requires_grad=True)\n",
    "\n",
    "# 建立一个计算图\n",
    "y = w*x + b \n",
    "\n",
    "# 计算梯度 \n",
    "# 疑问：梯度是如何计算的？？？？\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[-0.2777, -0.0171, -0.5040],\n",
      "        [-0.5327,  0.1511, -0.2322]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([ 0.2828, -0.4903], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 2. autograd例子\n",
    "\n",
    "# 创建二维的tensor\n",
    "# randn 表示从标准正态分布中抽取随机数\n",
    "x = torch.randn(10,3)\n",
    "y = torch.randn(10,2)\n",
    "\n",
    "# 建立一个全连接层\n",
    "linear = nn.Linear(3,2)\n",
    "print('w: ',linear.weight)\n",
    "print('b: ',linear.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以上为autograd的两个例子\n",
    "### 这里nn.linear的具体用法如下：\n",
    "作用：是对输入的数据进行线性变换 y=wx+b\n",
    "class torch.nn.Linear(in_features, out_features, bias=True)\n",
    "参数说明：\n",
    "    1. in_features : 每个输入样本的大小\n",
    "    2. out_features: 每个输出样本的大小\n",
    "    3. bias : 偏置项 默认为True 如果为False，则表示该层不学习偏置项\n",
    "\n",
    "weight -形状为(out_features  in_features)的模块中可学习的权值\n",
    "bias -形状为(out_features)的模块中可学习的偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.5604697465896606\n",
      "dw tensor([[ 0.0722, -0.1222, -0.3818],\n",
      "        [-0.0892,  0.1687, -0.0304]])\n",
      "db tensor([[ 0.0722, -0.1222, -0.3818],\n",
      "        [-0.0892,  0.1687, -0.0304]])\n",
      "梯度下降优化后的损失: 0.557906985282898\n"
     ]
    }
   ],
   "source": [
    "# 建立一个损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(),lr=0.01)\n",
    "# 前向传播，实际上就是将输入数据喂给神经网络\n",
    "pred = linear(x)\n",
    "# 计算损失\n",
    "loss = criterion(pred,y)\n",
    "print('loss:',loss.item())\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "print('dw',linear.weight.grad)\n",
    "print('db',linear.weight.grad)\n",
    "# 1-step gradient descent\n",
    "optimizer.step()\n",
    "pred = linear(x)\n",
    "loss = criterion(pred,y)\n",
    "print('梯度下降优化后的损失:',loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码，体现了经过反向传播后的损失得到减小，也就是梯度下降的作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面讲解上述代码中出现的具体函数：\n",
    "1. class torch.nn.MSELoss(size_average=True)\n",
    "创建一个用来衡量输入数据X与目标Y之间的均方差标准：\n",
    "$loss(x,y)=\\frac{1}{n}\\Sigma(x_i-y_i)^2$\n",
    "    * x,y均为任意形状，每个包含n个元素\n",
    "    * 如果设置参数size_average = False,则求出的x，y的差的平方和将不会除以n\n",
    "2. class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "用来实现随机梯度下降算法\n",
    "    * params 待优化的参数的iterable（类似于这种单词不要试着翻译，直接记住英文，便于理解），或者是预先定义好参数组的dict\n",
    "    * lr表示学习率\n",
    "    * 其原参数可选\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例子三 使用numpy加载数据\n",
    "这里主要讲解numpy和tensor之间的相互转换\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "由numpy转成tensor tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "由tensor转成numpy [[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1,2],[3,4]])\n",
    "# 将numpy转换成torch tensor\n",
    "y = torch.from_numpy(x)\n",
    "print('由numpy转成tensor',y)\n",
    "z = y.numpy()\n",
    "print('由tensor转成numpy',z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例子四 Input Pipieline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 下载CIFAR10数据集\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='/home/wangye/data',\n",
    "                                            train=True,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# 获取数据\n",
    "image , label = train_dataset[0]\n",
    "print(image.size())\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True)\n",
    "# 迭代之后，队列和线程便开始从文件加载数据\n",
    "data_iter = iter(train_loader)\n",
    "# 返回最小批量的数据\n",
    "images,labels = data_iter.next()\n",
    "# 正常情况下如何加载数据进行训练呢？\n",
    "for images , labels in train_loader:\n",
    "#     此处是详细的训练代码\n",
    "#     print(\"训练数据\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  函数的讲解：\n",
    "\n",
    "1. dset.CIFAR10(root, train=True, transform=None,target_transform=None, download=False)\n",
    "    * root是指定文件的下载路径，在上述代码中我们存储在了“/home/wangye/data”文件夹下。\n",
    "    * transform代表数据集要转换成什么形式，上述代码中我们转换成了tensor\n",
    "    * download表示是否下载，如果已经下载，则什么都不干\n",
    "    * train 表示是不是训练集 True : 训练集 False : 测试集\n",
    "2. class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=<function default_collate>, pin_memory=False, drop_last=False)\n",
    "    数据加载器，能够在数据集上面提供单进程或多进程的迭代器<br>\n",
    "    参数：\n",
    "    * dataset:加载的数据集\n",
    "    * batch_size:每个batch加载多少个样本\n",
    "    * shuffle:中文翻译为洗牌，也就是在每个Epoch时期，数据重新打乱\n",
    "    * num_workers:用多少个子进程加载数据，0表示仅仅使用主进程加载\n",
    "    * drop_last:如果数据集不能够整除batch_size，设置为True后，则可以删除最后一个不完整的batch，如果设置为False，最后一个不完整的数据集将被丢弃\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例子五 加载自定义的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "#         此处初始化函数,当本类的实例被创建的时候，该方法便会自动执行\n",
    "#         故此，可以在这里面写自己的初始化操作，然后传递参数\n",
    "        pass\n",
    "    def getitem(self,index):\n",
    "#         这里可以写读数据、预处理、返回数据的操作\n",
    "        pass\n",
    "\n",
    "# 下面就和例子四完全一样了\n",
    "# custom_dataset = CustomDataset()\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
    "#                                            batch_size=64, \n",
    "#                                            shuffle=True)\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例子六 与训练模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "# 下载和加载预训练的网络模型ResNet-18\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "# 微调网络模型的最顶层\n",
    "for param in resnet.parameters():\n",
    "#     print(param)\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 替换掉顶端以便微调\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features,100)\n",
    "\n",
    "# 向前传递\n",
    "images = torch.randn(64,3,224,224)\n",
    "outputs = resnet(images)\n",
    "print(outputs.size())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码讲解：\n",
    "1. torchvision.models\n",
    "    * 在这个model中，包含很多模型结构，分别是AlexNet、VGG、ResNet、SqueezeNet、DenseNet\n",
    "    * pretrained参数设置为True，则为加载在ImageNet上训练好的模型\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
