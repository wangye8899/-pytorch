{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 1. autograd例子\n",
    "# create tensors 创建张量\n",
    "\n",
    "# requires_grad:设置为True是允许精细的排除子图 提高计算效率\n",
    "x = torch.tensor(1.,requires_grad=True)\n",
    "w = torch.tensor(2.,requires_grad=True)\n",
    "b = torch.tensor(3.,requires_grad=True)\n",
    "\n",
    "# 建立一个计算图\n",
    "y = w*x + b \n",
    "\n",
    "# 计算梯度 \n",
    "# 疑问：梯度是如何计算的？？？？\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. autograd例子\n",
    "\n",
    "# 创建二维的tensor\n",
    "# randn 表示从标准正态分布中抽取随机数\n",
    "x = torch.randn(10,3)\n",
    "y = torch.randn(10,2)\n",
    "\n",
    "# 建立一个全连接层\n",
    "linear = nn.Linear(3,2)\n",
    "print('w: ',linear.weight)\n",
    "print('b: ',linear.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这里nn.linear的具体用法如下：\n",
    "作用：是对输入的数据进行线性变换 y=wx+b\n",
    "class torch.nn.Linear(in_features, out_features, bias=True)\n",
    "参数说明：\n",
    "    1. in_features : 每个输入样本的大小\n",
    "    2. out_features: 每个输出样本的大小\n",
    "    3. bias : 偏置项 默认为True 如果为False，则表示该层不学习偏置项\n",
    "\n",
    "weight -形状为(out_features  in_features)的模块中可学习的权值\n",
    "bias -形状为(out_features)的模块中可学习的偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.18735671043396\n",
      "dw tensor([[-0.7171, -0.1047, -0.4131],\n",
      "        [ 0.1479,  0.3800,  0.7006]])\n",
      "db tensor([[-0.7171, -0.1047, -0.4131],\n",
      "        [ 0.1479,  0.3800,  0.7006]])\n",
      "梯度下降优化后的损失: 1.1687290668487549\n"
     ]
    }
   ],
   "source": [
    "# 建立一个损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(),lr=0.01)\n",
    "# 前向传播，实际上就是将输入数据喂给神经网络\n",
    "pred = linear(x)\n",
    "# 计算损失\n",
    "loss = criterion(pred,y)\n",
    "print('loss:',loss.item())\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "print('dw',linear.weight.grad)\n",
    "print('db',linear.weight.grad)\n",
    "# 1-step gradient descent\n",
    "optimizer.step()\n",
    "pred = linear(x)\n",
    "loss = criterion(pred,y)\n",
    "print('梯度下降优化后的损失:',loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码，体现了经过反向传播后的损失得到减小，也就是梯度下降的作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面讲解上述代码中出现的具体函数：\n",
    "1. class torch.nn.MSELoss(size_average=True)\n",
    "创建一个用来衡量输入数据X与目标Y之间的均方差标准：\n",
    "$loss(x,y)=\\frac{1}{n}\\Sigma(x_i-y_i)^2$\n",
    "    * x,y均为任意形状，每个包含n个元素\n",
    "    * 如果设置参数size_average = False,则求出的x，y的差的平方和将不会除以n\n",
    "2. class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "用来实现随机梯度下降算法\n",
    "    * params 待优化的参数的iterable（类似于这种单词不要试着翻译，直接记住英文，便于理解），或者是预先定义好参数组的dict\n",
    "    * lr表示学习率\n",
    "    * 其原参数可选\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
